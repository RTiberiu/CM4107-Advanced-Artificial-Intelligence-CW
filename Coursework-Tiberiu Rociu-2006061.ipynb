{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> 074cc45f080b3cdf66b0553c93d3bd81380b5c7c
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_splits = {'train': 'Dataset_IMDB_Sentiment/plain_text/train-00000-of-00001.parquet', 'test': 'Dataset_IMDB_Sentiment/plain_text/test-00000-of-00001.parquet', 'unsupervised': 'Dataset_IMDB_Sentiment/plain_text/unsupervised-00000-of-00001.parquet'}\n",
<<<<<<< HEAD
    "imdb_train_df = pd.read_parquet(dataset_splits[\"train\"])\n",
    "imdb_test_df = pd.read_parquet(dataset_splits['test'])"
=======
    "imdb_train_df = pd.read_parquet(dataset_splits[\"train\"])"
>>>>>>> 074cc45f080b3cdf66b0553c93d3bd81380b5c7c
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 9,
>>>>>>> 074cc45f080b3cdf66b0553c93d3bd81380b5c7c
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
<<<<<<< HEAD
      "4  Oh, brother...after hearing about this ridicul...      0\n",
      "1640\n",
      "25000\n"
=======
      "4  Oh, brother...after hearing about this ridicul...      0\n"
>>>>>>> 074cc45f080b3cdf66b0553c93d3bd81380b5c7c
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Visualising the dataset \n",
    "print(imdb_train_df.head(5))\n",
    "\n",
    "print(len(imdb_train_df['text'][0]))\n",
    "print(len(imdb_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin # This allows the custom class to work with the Pipeline\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # Import the tokenize package and regex tokenizer\n",
    "from nltk.corpus import stopwords # Import stopwords for stopwords removal\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer # Import the snowball stemmer (also known as Porter2)\n",
    "import re # Import regex\n",
    "import numpy as np\n",
    "\n",
    "class pre_process(BaseEstimator, TransformerMixin): # TODO Further improve on this pre-processing\n",
    "    def __init__(self):\n",
    "      # Initialize objects for efficiency\n",
    "      self.stemmer = SnowballStemmer(\"english\")\n",
    "      self.lemmatizer = WordNetLemmatizer()\n",
    "      self.tokenizer = self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "      self.stopwords = stopwords.words(\"english\")\n",
    "      return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "      processed_text = []\n",
    "      for text in X:\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers NOTE: I added this\n",
    "        token_text = self.tokenizer.tokenize(text) \n",
    "        normalised_text = [token.lower() for token in token_text if token.isalpha()]\n",
    "\n",
    "        no_stopwords_text = [token for token in normalised_text if token not in self.stopwords]\n",
    "\n",
    "        # Using lemmatization or stemming TODO I should decide on one in the end\n",
    "        # processed_text += [[self.stemmer.stem(word) for word in swr_text]] # Applying stemmer\n",
    "        processed_text.append([self.lemmatizer.lemmatize(word) for word in no_stopwords_text]) # Applying the lemmatizer\n",
    "\n",
    "      #  Return tokens as a list of words (this ensure compatibility with FastText)\n",
    "      return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FastText class\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.vector_dim = 300\n",
    "        self.word_vectors = self.load_fasttext_vectors()\n",
    "\n",
    "    def load_fasttext_vectors(self):\n",
    "        # Loading the FastText word vectors from the .vec file\n",
    "        word_vectors = {}\n",
    "        with open(self.model_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                word_vectors[word] = vector\n",
    "        return word_vectors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        # Transform the pre-processed tokens into an average of word vectors\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            # Get the word vector from the tokens (from FastText)\n",
    "            word_vectors = [self.word_vectors[word] for word in token if word in self.word_vectors]\n",
    "\n",
    "            # Average a valid word vector or use a zero vector\n",
    "            if word_vectors:\n",
    "                vectors.append(np.mean(word_vectors, axis=0))\n",
    "            else:\n",
    "                print(f\"Debugging: Token didn't exist in the FastText vector dictionary: {token}\")\n",
    "                vectors.append(np.zeros(self.vector_dim))\n",
    "\n",
    "        return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify dataset shape to 3D to make it compatible with BiLSTM GRU model\n",
    "from keras.preprocessing.sequence import pad_sequences # Padding for the 3D array\n",
    "\n",
    "class ReshapeDatasetTo3D(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_sequence_length):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, tokens):\n",
    "        # Reshape the vectorized data into a 3D format: batch_size, sequence_length, vector_dim\n",
    "        # Pad sequences to ensure they have the same length\n",
    "        padded_data = pad_sequences(tokens, maxlen=self.max_sequence_length, dtype='float32', padding='post', truncating='post')\n",
    "        \n",
    "        # Reshape to match batch_size, sequence_length, vector_dim\n",
    "        reshaped_data = padded_data.reshape((padded_data.shape[0], self.max_sequence_length, -1))\n",
    "        \n",
    "        return reshaped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an example of the pre-processed pipeline output:\n",
      "\tThe original text (as tokens): I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\tThe pre-processed text: ['rented', 'curious', 'yellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', 'also', 'heard', 'first', 'seized', 'u', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'really', 'see', 'br', 'br', 'plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life', 'particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state', 'asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'men', 'br', 'br', 'kill', 'curious', 'yellow', 'year', 'ago', 'considered', 'pornographic', 'really', 'sex', 'nudity', 'scene', 'far', 'even', 'shot', 'like', 'cheaply', 'made', 'porno', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'film', 'br', 'br', 'commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america', 'curious', 'yellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'pun', 'intended', 'swedish', 'cinema', 'really', 'film', 'much', 'plot']\n",
      "\tThe FastText vector representation (sample): [-0.01422353 -0.02057058 -0.00514706 -0.02318824 -0.0206817   0.03414771\n",
      "  0.02831046  0.03746339  0.02238366  0.02086732]\n",
      "\tThe 3D reshaped data shape: (1, 300)\n",
      "\tThe 3D reshaped data representation (sample): [-0.01422353 -0.02057058 -0.00514706 -0.02318824 -0.0206817   0.03414771\n",
      "  0.02831046  0.03746339  0.02238366  0.02086732]\n"
     ]
    }
   ],
   "source": [
    "# Showcasing the pre processing and vectorization outputs used in the final model training pipelines\n",
    "from sklearn.pipeline import Pipeline # Adding the pipeline functionality\n",
    "\n",
    "# Showcasing the pre-process pipeline output\n",
    "pre_process_pipeline =  Pipeline([\n",
    "  ('prep', pre_process()) # Custom pre-processing method\n",
    "  ])\n",
    "\n",
    "pre_process_example = pre_process_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"Below is an example of the pre-processed pipeline output:\")\n",
    "print(f\"\\tThe original text (as tokens): {imdb_train_df['text'][0]}\")\n",
    "print(f\"\\tThe pre-processed text: {pre_process_example[0]}\")\n",
    "\n",
    "# Showcasing the FastText vectorization pipeline output\n",
    "fast_text_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\")), # FastText word to vector dictionary\n",
    "  ])\n",
    "\n",
    "fast_text_example = fast_text_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"\\tThe FastText vector representation (sample): {fast_text_example[0][:10]}\")\n",
    "\n",
    "# Showcasing the reshaped to 3D dataset pipeline output\n",
    "reshape_to_3D_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\")), # FastText word to vector dictionary\n",
    "  (\"reshape_dataset_to_3D\", ReshapeDatasetTo3D(max_sequence_length = 100)) # Reshape data to 3D to make it compatible with the BiLSTM GRU model\n",
    "  ])\n",
    "\n",
    "reshaped_to_3D_example = fast_text_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"\\tThe 3D reshaped data shape: {reshaped_to_3D_example.shape}\")\n",
    "print(f\"\\tThe 3D reshaped data representation (sample): {reshaped_to_3D_example[0][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first NLP pipeline\n",
    "first_NLP_pipeline = Pipeline([\n",
    "    (\"prep\", pre_process()),  # Custom pre-processing step\n",
    "    (\"fast_text_vectorizer\", FastTextVectorizer(model_path=\"Libraries/FastText/wiki-news-300d-1M.vec\")), # FastText word to vector dictionary\n",
    "    (\"reshape_dataset_to_3D\", ReshapeDatasetTo3D(max_sequence_length=100)), # Reshape data to 3D to make it compatible with the BiLSTM GRU model\n",
    "    # (\"model\", )  # Add the BiLSTM/GRU model \n",
    "])\n",
    "\n",
    "X_train = imdb_test_df['text'].values[:20]\n",
    "y_train = imdb_test_df['label'].values[:20]\n",
    "\n",
    "# Fit the pipeline\n",
    "first_NLP_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [101, 1045, 12524, 1045, 2572, 8025, 1011, 375...\n",
      "1        [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...\n",
      "2        [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...\n",
      "3        [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...\n",
      "4        [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...\n",
      "                               ...                        \n",
      "24995    [101, 1037, 2718, 2012, 1996, 2051, 2021, 2085...\n",
      "24996    [101, 1045, 2293, 2023, 3185, 2066, 2053, 2060...\n",
      "24997    [101, 2023, 2143, 1998, 2009, 1005, 1055, 8297...\n",
      "24998    [101, 1005, 1996, 7357, 1997, 6287, 18506, 100...\n",
      "24999    [101, 1996, 2466, 6401, 2105, 6287, 18506, 204...\n",
      "Name: tokenized_reviews, Length: 25000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: This might be my 2nd model to compare BiLSTM with. I might have to use ElectraTokenizer instead of FastText for this one.\n",
    "\n",
    "from sklearn.pipeline import Pipeline # Adding the pipeline functionality\n",
    "from transformers import ElectraTokenizer, ElectraModel \n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
=======
    "print(df.head(5))"
>>>>>>> 074cc45f080b3cdf66b0553c93d3bd81380b5c7c
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
