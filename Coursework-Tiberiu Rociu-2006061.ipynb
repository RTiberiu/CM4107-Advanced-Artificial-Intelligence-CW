{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_splits = {'train': 'Dataset_IMDB_Sentiment/plain_text/train-00000-of-00001.parquet', 'test': 'Dataset_IMDB_Sentiment/plain_text/test-00000-of-00001.parquet', 'unsupervised': 'Dataset_IMDB_Sentiment/plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "imdb_train_df = pd.read_parquet(dataset_splits[\"train\"])\n",
    "imdb_test_df = pd.read_parquet(dataset_splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n",
      "1640\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Visualising the dataset \n",
    "print(imdb_train_df.head(5))\n",
    "\n",
    "print(len(imdb_train_df['text'][0]))\n",
    "print(len(imdb_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin # This allows the custom class to work with the Pipeline\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # Import the tokenize package and regex tokenizer\n",
    "from nltk.corpus import stopwords # Import stopwords for stopwords removal\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer # Import the snowball stemmer (also known as Porter2)\n",
    "import re # Import regex\n",
    "import numpy as np\n",
    "\n",
    "class pre_process(BaseEstimator, TransformerMixin): # TODO Further improve on this pre-processing\n",
    "    def __init__(self):\n",
    "      # Initialize objects for efficiency\n",
    "      self.stemmer = SnowballStemmer(\"english\")\n",
    "      self.lemmatizer = WordNetLemmatizer()\n",
    "      self.tokenizer = self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "      self.stopwords = stopwords.words(\"english\")\n",
    "      return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "      processed_text = []\n",
    "      for text in X:\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers NOTE: I added this\n",
    "        token_text = self.tokenizer.tokenize(text) \n",
    "        normalised_text = [token.lower() for token in token_text if token.isalpha()]\n",
    "\n",
    "        no_stopwords_text = [token for token in normalised_text if token not in self.stopwords]\n",
    "\n",
    "        # Using lemmatization or stemming TODO I should decide on one in the end\n",
    "        # processed_text += [[self.stemmer.stem(word) for word in swr_text]] # Applying stemmer\n",
    "        processed_text.append([self.lemmatizer.lemmatize(word) for word in no_stopwords_text]) # Applying the lemmatizer\n",
    "\n",
    "      #  Return tokens as a list of words (this ensure compatibility with FastText)\n",
    "      return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FastText class\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_path, max_sequence_length):\n",
    "        self.model_path = model_path\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.vector_dim = 300\n",
    "        self.word_vectors = self.load_fasttext_vectors()\n",
    "\n",
    "    def load_fasttext_vectors(self):\n",
    "        # Loading the FastText word vectors from the .vec file\n",
    "        word_vectors = {}\n",
    "        with open(self.model_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                word_vectors[word] = vector\n",
    "        return word_vectors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        # Convert each tokenized text into a list of word vectors\n",
    "        tokenized_vectors = [\n",
    "            [\n",
    "                self.word_vectors[word] if word in self.word_vectors else np.zeros(self.vector_dim)\n",
    "                for word in token\n",
    "            ]\n",
    "            for token in tokens\n",
    "        ]\n",
    "\n",
    "        # Reshape data to 3D to make it compatible with the BiLSTM GRU model\n",
    "        # Pad sequences to ensure consistent dimensions: sequence length, vector dim\n",
    "        padded_data = pad_sequences(\n",
    "            [np.array(seq) for seq in tokenized_vectors],\n",
    "            maxlen=self.max_sequence_length,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "        )\n",
    "\n",
    "        return np.array(padded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an example of the pre-processed pipeline output:\n",
      "\tThe original text (as tokens): I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\tThe pre-processed text: ['rented', 'curious', 'yellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', 'also', 'heard', 'first', 'seized', 'u', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'really', 'see', 'br', 'br', 'plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life', 'particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state', 'asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'men', 'br', 'br', 'kill', 'curious', 'yellow', 'year', 'ago', 'considered', 'pornographic', 'really', 'sex', 'nudity', 'scene', 'far', 'even', 'shot', 'like', 'cheaply', 'made', 'porno', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'film', 'br', 'br', 'commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america', 'curious', 'yellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'pun', 'intended', 'swedish', 'cinema', 'really', 'film', 'much', 'plot']\n",
      "\tThe 3D reshaped data shape: (1, 250, 300)\n",
      "\tThe 3D reshaped data representation (sample): [[ 0.1991  0.0065 -0.0965 ...  0.1122 -0.0166 -0.1085]\n",
      " [-0.1178 -0.1638  0.0755 ...  0.1817  0.2032  0.2252]\n",
      " [-0.049   0.032   0.024  ... -0.04    0.1997  0.0506]\n",
      " ...\n",
      " [ 0.0092 -0.0123 -0.0336 ...  0.1672 -0.0127  0.0247]\n",
      " [-0.0075 -0.0557 -0.0249 ...  0.0717  0.0584  0.0515]\n",
      " [-0.0764  0.0451 -0.0883 ...  0.1588  0.0052 -0.0104]]\n"
     ]
    }
   ],
   "source": [
    "# Showcasing the pre processing and vectorization outputs used in the final model training pipelines\n",
    "from sklearn.pipeline import Pipeline # Adding the pipeline functionality\n",
    "\n",
    "# Showcasing the pre-process pipeline output\n",
    "pre_process_pipeline =  Pipeline([\n",
    "  ('prep', pre_process()) # Custom pre-processing method\n",
    "  ])\n",
    "\n",
    "pre_process_example = pre_process_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"Below is an example of the pre-processed pipeline output:\")\n",
    "print(f\"\\tThe original text (as tokens): {imdb_train_df['text'][0]}\")\n",
    "print(f\"\\tThe pre-processed text: {pre_process_example[0]}\")\n",
    "\n",
    "# Showcasing the FastText vectorization pipeline output with 3D shape data\n",
    "fast_text_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(\n",
    "    model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\",\n",
    "    max_sequence_length = 250\n",
    "    )), # FastText word to vector dictionary\n",
    "  ])\n",
    "\n",
    "fast_text_pipeline = fast_text_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"\\tThe 3D reshaped data shape: {fast_text_pipeline.shape}\")\n",
    "print(f\"\\tThe 3D reshaped data representation (sample): {fast_text_pipeline[0][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens: 123.56692\n",
      "Maximum number of tokens: 1442\n",
      "Minimum number of tokens: 4\n",
      "90th percentile token length: 246\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average tokens per imdb review. \n",
    "# This average will be used for the max sequence length in FastText, to avoid truncating and losing potential semantics information\n",
    "pre_process_example = pre_process_pipeline.fit_transform(imdb_train_df['text'].values)\n",
    "\n",
    "# Calculate token counts for each pre-processed text\n",
    "token_lengths = [len(tokens) for tokens in pre_process_example]\n",
    "\n",
    "# Calculate the statistics\n",
    "average_tokens = sum(token_lengths) / len(token_lengths)\n",
    "max_tokens = max(token_lengths)\n",
    "min_tokens = min(token_lengths)\n",
    "max_seq_len_90th  = int(np.percentile(token_lengths, 90))\n",
    "\n",
    "print(f\"Average number of tokens: {average_tokens}\")\n",
    "print(f\"Maximum number of tokens: {max_tokens}\")\n",
    "print(f\"Minimum number of tokens: {min_tokens}\")\n",
    "print(f\"90th percentile token length: {max_seq_len_90th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the model wrapped in a custom Keras model to work with the Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, GRU, Dense, Dropout\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class BiLSTM_GRU_Model(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape, num_classes, dropout_rate):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(32, return_sequences=True), input_shape=self.input_shape))\n",
    "        model.add(GRU(16))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=5, batch_size=16, verbose=1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return predictions[:, 1] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.9998 - loss: 9.9003e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.9969 - loss: 0.0099\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.9985 - loss: 0.0044\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.9985 - loss: 0.0049\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.9993 - loss: 0.0027\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.9995 - loss: 0.0024\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.9992 - loss: 0.0031\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.9968 - loss: 0.0084\n",
      "Epoch 9/10\n",
      "\u001b[1m505/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.9985 - loss: 0.0058"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "max_sequence_length = 250 # This is the 90th percentile review token length score (rounded)\n",
    "input_shape = (max_sequence_length, 300)  # Match the reshaped dimensions: max_sequence_length x vector_dim\n",
    "\n",
    "bilstm_gru_NLP_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(\n",
    "    model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\",\n",
    "    max_sequence_length = max_sequence_length\n",
    "    )), # FastText word to vector dictionary\n",
    "    (\"model\", BiLSTM_GRU_Model(input_shape=input_shape, num_classes=2, dropout_rate=0.3))  # BiLSTM GRU model\n",
    "  ])\n",
    "\n",
    "X_train = imdb_train_df['text'].values\n",
    "y_train = imdb_train_df['label'].values\n",
    "\n",
    "# Fit the pipeline\n",
    "first_NLP_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make the predictions\n",
    "X_test = imdb_test_df['text'].values\n",
    "y_pred = first_NLP_pipeline.predict(X_test)\n",
    "y_test = imdb_test_df['label'].values\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Compare predictions with the actual ones\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [101, 1045, 12524, 1045, 2572, 8025, 1011, 375...\n",
      "1        [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...\n",
      "2        [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...\n",
      "3        [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...\n",
      "4        [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...\n",
      "                               ...                        \n",
      "24995    [101, 1037, 2718, 2012, 1996, 2051, 2021, 2085...\n",
      "24996    [101, 1045, 2293, 2023, 3185, 2066, 2053, 2060...\n",
      "24997    [101, 2023, 2143, 1998, 2009, 1005, 1055, 8297...\n",
      "24998    [101, 1005, 1996, 7357, 1997, 6287, 18506, 100...\n",
      "24999    [101, 1996, 2466, 6401, 2105, 6287, 18506, 204...\n",
      "Name: tokenized_reviews, Length: 25000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: This might be my 2nd model to compare BiLSTM with. I might have to use ElectraTokenizer instead of FastText for this one.\n",
    "\n",
    "from sklearn.pipeline import Pipeline # Adding the pipeline functionality\n",
    "from transformers import ElectraTokenizer, ElectraModel \n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "model = ElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
