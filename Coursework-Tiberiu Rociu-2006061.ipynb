{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDb reviews using BiLSTM and Electra\n",
    "Author: Tiberiu Rociu  \n",
    "Student ID: 2006061\n",
    "\n",
    "# Introduction\n",
    "\n",
    "    TODO: Add introduction\n",
    "\n",
    "# Representation Learning\n",
    "\n",
    "    TODO: Add representation plan\n",
    "\n",
    "# Algorithms\n",
    "\n",
    "    TODO: Add algorithm description\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "    TODO: Add evaluation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_splits = {'train': 'Dataset_IMDB_Sentiment/plain_text/train-00000-of-00001.parquet', 'test': 'Dataset_IMDB_Sentiment/plain_text/test-00000-of-00001.parquet', 'unsupervised': 'Dataset_IMDB_Sentiment/plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "imdb_train_df = pd.read_parquet(dataset_splits[\"train\"])\n",
    "imdb_test_df = pd.read_parquet(dataset_splits['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
      "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
      "2  If only to avoid making this type of film in t...      0\n",
      "3  This film was probably inspired by Godard's Ma...      0\n",
      "4  Oh, brother...after hearing about this ridicul...      0\n",
      "1640\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Visualising the dataset \n",
    "print(imdb_train_df.head(5))\n",
    "\n",
    "print(len(imdb_train_df['text'][0]))\n",
    "print(len(imdb_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tiberiu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin # This allows the custom class to work with the Pipeline\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer # Import the tokenize package and regex tokenizer\n",
    "from nltk.corpus import stopwords # Import stopwords for stopwords removal\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer # Import the snowball stemmer (also known as Porter2)\n",
    "import re # Import regex\n",
    "import numpy as np\n",
    "\n",
    "class pre_process(BaseEstimator, TransformerMixin): # TODO Further improve on this pre-processing\n",
    "    def __init__(self):\n",
    "      # Initialize objects for efficiency\n",
    "      self.stemmer = SnowballStemmer(\"english\")\n",
    "      self.lemmatizer = WordNetLemmatizer()\n",
    "      self.tokenizer = self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "      self.stopwords = stopwords.words(\"english\")\n",
    "      return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "      processed_text = []\n",
    "      for text in X:\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        token_text = self.tokenizer.tokenize(text) \n",
    "        normalised_text = [token.lower() for token in token_text if token.isalpha()]\n",
    "\n",
    "        no_stopwords_text = [token for token in normalised_text if token not in self.stopwords]\n",
    "\n",
    "        # Using lemmatization or stemming TODO I should decide on one in the end\n",
    "        # processed_text += [[self.stemmer.stem(word) for word in swr_text]] # Applying stemmer\n",
    "        processed_text.append([self.lemmatizer.lemmatize(word) for word in no_stopwords_text]) # Applying the lemmatizer\n",
    "\n",
    "      #  Return tokens as a list of words (this ensure compatibility with FastText)\n",
    "      return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# The FastText class\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_path, max_sequence_length):\n",
    "        self.model_path = model_path\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.vector_dim = 300\n",
    "        self.word_vectors = self.load_fasttext_vectors()\n",
    "\n",
    "    def load_fasttext_vectors(self):\n",
    "        # Loading the FastText word vectors from the .vec file\n",
    "        word_vectors = {}\n",
    "        with open(self.model_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                word_vectors[word] = vector\n",
    "        return word_vectors\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, tokens):\n",
    "        # Convert each tokenized text into a list of word vectors\n",
    "        tokenized_vectors = [\n",
    "            [\n",
    "                self.word_vectors[word] if word in self.word_vectors else np.zeros(self.vector_dim)\n",
    "                for word in token\n",
    "            ]\n",
    "            for token in tokens\n",
    "        ]\n",
    "\n",
    "        # Reshape data to 3D to make it compatible with the BiLSTM GRU model\n",
    "        # Pad sequences to ensure consistent dimensions: sequence length, vector dim\n",
    "        padded_data = pad_sequences(\n",
    "            [np.array(seq) for seq in tokenized_vectors],\n",
    "            maxlen=self.max_sequence_length,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "        )\n",
    "\n",
    "        return np.array(padded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an example of the pre-processed pipeline output:\n",
      "\tThe original text (as tokens): I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "\tThe pre-processed text: ['rented', 'curious', 'yellow', 'video', 'store', 'controversy', 'surrounded', 'first', 'released', 'also', 'heard', 'first', 'seized', 'u', 'custom', 'ever', 'tried', 'enter', 'country', 'therefore', 'fan', 'film', 'considered', 'controversial', 'really', 'see', 'br', 'br', 'plot', 'centered', 'around', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'want', 'learn', 'everything', 'life', 'particular', 'want', 'focus', 'attention', 'making', 'sort', 'documentary', 'average', 'swede', 'thought', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'state', 'asking', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politics', 'sex', 'drama', 'teacher', 'classmate', 'married', 'men', 'br', 'br', 'kill', 'curious', 'yellow', 'year', 'ago', 'considered', 'pornographic', 'really', 'sex', 'nudity', 'scene', 'far', 'even', 'shot', 'like', 'cheaply', 'made', 'porno', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'even', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'film', 'br', 'br', 'commend', 'filmmaker', 'fact', 'sex', 'shown', 'film', 'shown', 'artistic', 'purpose', 'rather', 'shock', 'people', 'make', 'money', 'shown', 'pornographic', 'theater', 'america', 'curious', 'yellow', 'good', 'film', 'anyone', 'wanting', 'study', 'meat', 'potato', 'pun', 'intended', 'swedish', 'cinema', 'really', 'film', 'much', 'plot']\n",
      "\tThe 3D reshaped data shape: (1, 250, 300)\n",
      "\tThe 3D reshaped data representation (sample): [[ 0.1991  0.0065 -0.0965 ...  0.1122 -0.0166 -0.1085]\n",
      " [-0.1178 -0.1638  0.0755 ...  0.1817  0.2032  0.2252]\n",
      " [-0.049   0.032   0.024  ... -0.04    0.1997  0.0506]\n",
      " ...\n",
      " [ 0.0092 -0.0123 -0.0336 ...  0.1672 -0.0127  0.0247]\n",
      " [-0.0075 -0.0557 -0.0249 ...  0.0717  0.0584  0.0515]\n",
      " [-0.0764  0.0451 -0.0883 ...  0.1588  0.0052 -0.0104]]\n"
     ]
    }
   ],
   "source": [
    "# Showcasing the pre processing and vectorization outputs used in the final model training pipelines\n",
    "from sklearn.pipeline import Pipeline # Adding the pipeline functionality\n",
    "\n",
    "# Showcasing the pre-process pipeline output\n",
    "pre_process_pipeline =  Pipeline([\n",
    "  ('prep', pre_process()) # Custom pre-processing method\n",
    "  ])\n",
    "\n",
    "pre_process_example = pre_process_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"Below is an example of the pre-processed pipeline output:\")\n",
    "print(f\"\\tThe original text (as tokens): {imdb_train_df['text'][0]}\")\n",
    "print(f\"\\tThe pre-processed text: {pre_process_example[0]}\")\n",
    "\n",
    "# Showcasing the FastText vectorization pipeline output with 3D shape data\n",
    "fast_text_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(\n",
    "    model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\",\n",
    "    max_sequence_length = 250\n",
    "    )), # FastText word to vector dictionary\n",
    "  ])\n",
    "\n",
    "fast_text_pipeline = fast_text_pipeline.fit_transform(imdb_train_df['text'][:1])\n",
    "print(f\"\\tThe 3D reshaped data shape: {fast_text_pipeline.shape}\")\n",
    "print(f\"\\tThe 3D reshaped data representation (sample): {fast_text_pipeline[0][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens: 123.56692\n",
      "Maximum number of tokens: 1442\n",
      "Minimum number of tokens: 4\n",
      "90th percentile token length: 246\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average tokens per imdb review. \n",
    "# This average will be used for the max sequence length in FastText, to avoid truncating and losing potential semantics information\n",
    "pre_process_example = pre_process_pipeline.fit_transform(imdb_train_df['text'].values)\n",
    "\n",
    "# Calculate token counts for each pre-processed text\n",
    "token_lengths = [len(tokens) for tokens in pre_process_example]\n",
    "\n",
    "# Calculate the statistics\n",
    "average_tokens = sum(token_lengths) / len(token_lengths)\n",
    "max_tokens = max(token_lengths)\n",
    "min_tokens = min(token_lengths)\n",
    "max_seq_len_90th  = int(np.percentile(token_lengths, 90))\n",
    "\n",
    "print(f\"Average number of tokens: {average_tokens}\")\n",
    "print(f\"Maximum number of tokens: {max_tokens}\")\n",
    "print(f\"Minimum number of tokens: {min_tokens}\")\n",
    "print(f\"90th percentile token length: {max_seq_len_90th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the model wrapped in a custom Keras model to work with the Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, GRU, Dense, Dropout\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from keras import Input\n",
    "\n",
    "class BiLSTM_GRU_Model(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_shape, num_classes, dropout_rate):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=self.input_shape))\n",
    "        model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "        model.add(GRU(16))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=10, batch_size=16, verbose=1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        return (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len x_train 25000\n",
      "Len y_train 25000\n",
      "Shape X_train (25000,)\n",
      "Shape y_train (25000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "max_sequence_length = 250 # This is the 90th percentile review token length score (rounded)\n",
    "input_shape = (max_sequence_length, 300)  # Match the reshaped dimensions: max_sequence_length x vector_dim\n",
    "\n",
    "bilstm_gru_NLP_pipeline = Pipeline([\n",
    "  (\"prep\", pre_process()), # Custom pre-processing method\n",
    "  (\"fast_text_vectorizer\", FastTextVectorizer(\n",
    "    model_path = \"Libraries/FastText/wiki-news-300d-1M.vec\",\n",
    "    max_sequence_length = max_sequence_length\n",
    "    )), # FastText word to vector dictionary\n",
    "    (\"model\", BiLSTM_GRU_Model(input_shape=input_shape, num_classes=1, dropout_rate=0.3))  # BiLSTM GRU model\n",
    "  ])\n",
    "\n",
    "X_train = imdb_train_df['text'].values\n",
    "y_train = imdb_train_df['label'].values\n",
    "\n",
    "print(f\"Len x_train {len(X_train)}\")\n",
    "print(f\"Len y_train {len(y_train)}\")\n",
    "print(f\"Shape X_train {X_train.shape}\")\n",
    "print(f\"Shape y_train {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 54ms/step - accuracy: 0.5113 - loss: 0.6920\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 53ms/step - accuracy: 0.5388 - loss: 0.6816\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 53ms/step - accuracy: 0.6932 - loss: 0.5552\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.8733 - loss: 0.3108\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 53ms/step - accuracy: 0.8860 - loss: 0.2808\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 55ms/step - accuracy: 0.8952 - loss: 0.2637\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 53ms/step - accuracy: 0.9066 - loss: 0.2397\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.9154 - loss: 0.2220\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 51ms/step - accuracy: 0.9238 - loss: 0.2010\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 49ms/step - accuracy: 0.9297 - loss: 0.1858\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;, pre_process()),\n",
       "                (&#x27;fast_text_vectorizer&#x27;,\n",
       "                 FastTextVectorizer(max_sequence_length=250,\n",
       "                                    model_path=&#x27;Libraries/FastText/wiki-news-300d-1M.vec&#x27;)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 BiLSTM_GRU_Model(dropout_rate=0.3, input_shape=(250, 300),\n",
       "                                  num_classes=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;prep&#x27;, pre_process()),\n",
       "                (&#x27;fast_text_vectorizer&#x27;,\n",
       "                 FastTextVectorizer(max_sequence_length=250,\n",
       "                                    model_path=&#x27;Libraries/FastText/wiki-news-300d-1M.vec&#x27;)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 BiLSTM_GRU_Model(dropout_rate=0.3, input_shape=(250, 300),\n",
       "                                  num_classes=1))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">pre_process</label><div class=\"sk-toggleable__content \"><pre>pre_process()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">FastTextVectorizer</label><div class=\"sk-toggleable__content \"><pre>FastTextVectorizer(max_sequence_length=250,\n",
       "                   model_path=&#x27;Libraries/FastText/wiki-news-300d-1M.vec&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">BiLSTM_GRU_Model</label><div class=\"sk-toggleable__content \"><pre>BiLSTM_GRU_Model(dropout_rate=0.3, input_shape=(250, 300), num_classes=1)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep', pre_process()),\n",
       "                ('fast_text_vectorizer',\n",
       "                 FastTextVectorizer(max_sequence_length=250,\n",
       "                                    model_path='Libraries/FastText/wiki-news-300d-1M.vec')),\n",
       "                ('model',\n",
       "                 BiLSTM_GRU_Model(dropout_rate=0.3, input_shape=(250, 300),\n",
       "                                  num_classes=1))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline\n",
    "bilstm_gru_NLP_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step\n",
      "Predictions: [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Accuracy: 0.89\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.89     12500\n",
      "           1       0.90      0.87      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the predictions\n",
    "X_test = imdb_test_df['text'].values\n",
    "y_pred = bilstm_gru_NLP_pipeline.predict(X_test)\n",
    "y_test = imdb_test_df['label'].values\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Compare predictions with the actual ones\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING THE 2ND PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Transforming into HuggingFace Dataset objects\n",
    "imdb_train_hf = Dataset.from_pandas(imdb_train_df[['text', 'label']])\n",
    "imdb_test_hf = Dataset.from_pandas(imdb_test_df[['text', 'label']])\n",
    "\n",
    "# Loading the Electra tokeniser to use for the Electra model\n",
    "electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electra tokenising function\n",
    "def electra_tokenize_function(data):\n",
    "    return tokenizer(data['text'], padding = \"max_length\", truncation = True, return_attention_mask = True, max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:44<00:00, 560.35 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:43<00:00, 571.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenising the dataset\n",
    "imdb_train_hf = imdb_train_hf.map(electra_tokenize_function, batched=True)\n",
    "imdb_test_hf = imdb_test_hf.map(electra_tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the Electra model and setting the training parameters\n",
    "electra_model = ElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator', num_labels=2)\n",
    "\n",
    "electra_training_arguments = TrainingArguments(\n",
    "    output_dir = './results',  \n",
    "    eval_strategy = \"epoch\", \n",
    "    learning_rate = 2e-5,  \n",
    "    per_device_train_batch_size = 32, # Making the model use the GPU for training\n",
    "    per_device_eval_batch_size = 32, \n",
    "    num_train_epochs = 7,\n",
    "    weight_decay = 0.01, \n",
    "    warmup_steps = 200,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 500,\n",
    "    save_steps = 500,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "electra_trainer = Trainer(\n",
    "    model = electra_model,\n",
    "    args = electra_training_arguments,\n",
    "    train_dataset = imdb_train_hf,\n",
    "    eval_dataset = imdb_test_hf,\n",
    "    processing_class = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# Making sure the script recognizes the GPU, to ensure the model is in fact training on it\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 500/5474 [01:01<09:47,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4595, 'grad_norm': 8.129602432250977, 'learning_rate': 1.8866135760333716e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 781/5474 [01:34<08:59,  8.69it/s]\n",
      " 14%|█▍        | 783/5474 [02:10<10:50:44,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22111102938652039, 'eval_runtime': 35.6266, 'eval_samples_per_second': 701.723, 'eval_steps_per_second': 21.95, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1000/5474 [02:36<08:47,  8.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2467, 'grad_norm': 3.4743406772613525, 'learning_rate': 1.697004171406902e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1500/5474 [03:35<07:48,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2097, 'grad_norm': 7.339658260345459, 'learning_rate': 1.5073947667804325e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 1563/5474 [03:43<07:30,  8.69it/s]\n",
      " 29%|██▊       | 1565/5474 [04:19<9:10:34,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2104008048772812, 'eval_runtime': 36.1802, 'eval_samples_per_second': 690.986, 'eval_steps_per_second': 21.614, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2000/5474 [05:11<06:59,  8.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1768, 'grad_norm': 10.491612434387207, 'learning_rate': 1.3181645809632159e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2345/5474 [05:52<05:59,  8.69it/s]\n",
      " 43%|████▎     | 2347/5474 [06:28<7:08:43,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21426326036453247, 'eval_runtime': 35.2065, 'eval_samples_per_second': 710.096, 'eval_steps_per_second': 22.212, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2500/5474 [06:46<05:50,  8.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1644, 'grad_norm': 13.929616928100586, 'learning_rate': 1.1285551763367465e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2528/5474 [06:49<05:47,  8.48it/s]"
     ]
    }
   ],
   "source": [
    "# Train the Electra model to fine tune it to the IMDB dataset\n",
    "# NOTE FOR MARKING PURPOSES: Below this code block there will be code that will load the trained model already.\n",
    "# Retraining the model will probably take a long time if a dedicated GPU is not used.\n",
    "electra_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./electra_custom_trained_model\\\\tokenizer_config.json',\n",
       " './electra_custom_trained_model\\\\special_tokens_map.json',\n",
       " './electra_custom_trained_model\\\\vocab.txt',\n",
       " './electra_custom_trained_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the Electra model\n",
    "electra_model.save_pretrained('./electra_custom_trained_model')\n",
    "electra_tokenizer.save_pretrained('./electra_custom_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the custom Electra model and tokenizer\n",
    "electra_custom_model = ElectraForSequenceClassification.from_pretrained('./electra_custom_trained_model')\n",
    "electra_custom_tokenizer = ElectraTokenizer.from_pretrained('./electra_custom_trained_model')\n",
    "\n",
    "# Create a new Trainer instance with the custom model\n",
    "custom_electra_trainer = Trainer(\n",
    "    model = electra_custom_model,\n",
    "    processing_class = electra_custom_tokenizer,\n",
    "    args = electra_training_arguments,\n",
    "    eval_dataset = imdb_test_hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:17<00:00, 43.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.41617077589035034, 'eval_model_preparation_time': 0.0015, 'eval_runtime': 17.8484, 'eval_samples_per_second': 1400.686, 'eval_steps_per_second': 43.813}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "results = custom_electra_trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:35<00:00, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Electra model accuracy on the test dataset: 92.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating the predictions on the test dataset\n",
    "def predict_with_trainer(trainer, dataset):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = predictions.predictions.argmax(axis=1)\n",
    "    return preds, predictions.label_ids\n",
    "\n",
    "predictions, labels = predict_with_trainer(custom_electra_trainer, imdb_test_hf)\n",
    "\n",
    "# Showing the model accuracy\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "print(f\"Custom Electra model accuracy on the test dataset: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.88      0.91     12500\n",
      "    Positive       0.89      0.94      0.91     12500\n",
      "\n",
      "    accuracy                           0.91     25000\n",
      "   macro avg       0.91      0.91      0.91     25000\n",
      "weighted avg       0.91      0.91      0.91     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing the classification report\n",
    "report = classification_report(labels, predictions, target_names=['Negative', 'Positive'])\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
